# -*- coding: utf-8 -*-
"""AssignmentNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vP__nqiMzfX42IJdE4rz_CqeBOkBb5wc

# **IMPORTS**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.losses import MeanSquaredError
from keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.metrics import RootMeanSquaredError
from keras.layers import *
import sklearn
from sklearn.preprocessing import StandardScaler, MinMaxScaler

"""# **DATA** **LOADING**"""

df = pd.read_excel("Cluster_Data_HDBSCAN.xlsx", "cluster4")

df.head()

df.columns = df.columns.astype(str)

time = [i+1 for i in range(len(df))]
time = np.array(time)

n_future = 1
n_past = 5
col = 17
split = 0.8
span = col

actuals = df[:].to_numpy().astype('float64').T[col]
mu = np.mean(actuals)
sd = np.std(actuals)
min = np.min(actuals)
max = np.max(actuals)
actuals = (actuals-min) / (max-min)
# actuals = (actuals - mu) / sd**2

X = []
Y = []

for i in range(n_past, len(actuals) - n_future +1):
    X.append(actuals[i - n_past:i])
    Y.append(actuals[i + n_future - 1:i + n_future])

X, Y = np.array(X).astype('float64'), np.array(Y).astype('float64')

trainX = X[:int(len(X)*split)]
trainY = Y[:int(len(X)*split)]
trainX.shape, trainY.shape

testX = X[int(len(X)*split):]
testY = Y[int(len(X)*split):]
testX.shape, testY.shape

trainX = trainX.reshape(-1, n_past, 1)
trainY = trainY.reshape(-1,)

testX = testX.reshape(-1, n_past, 1)
testY = testY.reshape(-1,)

trainX.shape, trainY.shape

testX.shape, testY.shape

"""# **MODEL** **BUILDING**"""

model1 = Sequential()
model1.add(Input((n_past, 1)))
model1.add(LSTM(80))
model1.add(Dense(20, 'selu'))
model1.add(Dense(1, 'linear'))

model1.summary()

chp1 = ModelCheckpoint('model1/', save_best_only=True)
model1.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])

# fit the model
history = model1.fit(trainX, trainY, epochs=30, validation_split=0.20, callbacks = [chp1])

plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()

model1.save_weights("model_weights1.h5")

model2 = Sequential()
model2.add(Input((n_past, 1)))
model2.add(Conv1D(80, kernel_size = 2))
model2.add(MaxPooling1D(2))
model2.add(Flatten())
model2.add(Dense(20, 'selu'))
model2.add(Dense(1, 'linear'))

model2.summary()

chp2 = ModelCheckpoint('model2/', save_best_only=True)
model2.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])

# fit the model
history = model2.fit(trainX, trainY, epochs=30, validation_split=0.20, callbacks = [chp2])

plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()

model2.save_weights("model_weights2.h5")

model3 = Sequential()
model3.add(Input((n_past, 1)))
model3.add(LSTM(80, return_sequences=True))
model3.add(LSTM(80, 'sigmoid', return_sequences=False))
model3.add(Flatten())
model3.add(Dense(40, 'selu'))
model3.add(Dense(15, 'selu'))
model3.add(Dense(1, 'linear'))

model3.summary()

chp3 = ModelCheckpoint('model3/', save_best_only=True)
model3.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])

# fit the model
history = model3.fit(trainX, trainY, epochs=30, validation_split=0.20, callbacks = [chp3])

plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()

model3.save_weights("model_weights3.h5")

model4 = Sequential()
model4.add(Input((n_past, 1)))
model4.add(Conv1D(80, 3))
model4.add(Conv1D(60, 2, activation='sigmoid'))
model4.add(MaxPooling1D(2))
model4.add(LSTM(50, return_sequences=True))
model4.add(LSTM(50, 'sigmoid', return_sequences=False))
model4.add(Flatten())
model4.add(Dense(30, 'sigmoid'))
model4.add(Dense(10, 'selu'))
model4.add(Dense(1, 'linear'))

model4.summary()

chp4 = ModelCheckpoint('model4/', save_best_only=True)
model4.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])

# fit the model
history = model4.fit(trainX, trainY, epochs=40, validation_split=0.20, callbacks = [chp4])

plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()

model4.save_weights("model_weights4.h5")

model5 = Sequential()
model5.add(Input((n_past, 1)))
model5.add(Flatten())
model5.add(Dense(90, 'selu'))
model5.add(Dense(80, 'sigmoid'))
model5.add(Dense(50, 'selu'))
model5.add(Dense(25, 'sigmoid'))
model5.add(Dense(15, 'selu'))
model5.add(Dense(5, 'relu'))
model5.add(Dense(1))

model5.summary()

chp5 = ModelCheckpoint('model5/', save_best_only=True)
model5.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])

# fit the model
history = model5.fit(trainX, trainY, epochs=40, validation_split=0.20, callbacks = [chp5])

plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()

model5.save_weights("model_weights5.h5")

"""# **MODEL** **TESTING**"""

models = [model1, model2, model3, model4, model5]

Model_number = 3
model = models[Model_number-1]
wt_no = "model_weights" + str(Model_number) + ".h5"
model.load_weights(wt_no)
# type(model)

train_predictions = model.predict(trainX).flatten()
train_results = pd.DataFrame(data={'Train Predictions':train_predictions, 'Original Train Predictions':trainY})
train_results

test_predictions = model.predict(testX).flatten()
test_results = pd.DataFrame(data={'Test Predictions':test_predictions, 'Actuals':testY})
test_results

actY = testY*(max-min) + min
tp = (test_predictions - np.min(test_predictions))/(np.min(test_predictions)-np.max(test_predictions))
mx = np.max(actY)
mi = np.min(actY)
predY = mi + tp * (mx-mi)
plt.plot(2-predY, 'r')
plt.plot(actY, 'g')
plt.show()

act2Y = trainY*(max-min) + min
tp2 = (train_predictions - np.min(train_predictions))/(np.min(train_predictions)-np.max(train_predictions))
mx = np.max(act2Y)
mi = np.min(act2Y)
predY2 = mi + tp2 * (mx-mi)
plt.plot(2-predY2, 'r')
plt.plot(act2Y, 'g')
plt.show()

len(predY2)+len(predY)

total = np.append(act2Y, actY)
time = [i for i in range(0, len(X))]
plt.plot(time[0 :len(predY2)], 2-predY2, 'r')
plt.plot(time[len(predY2):len(predY2)+len(predY)], 2-predY, 'b')
plt.plot(total, 'g')
plt.legend(['Train Predictions', 'Test Predictions', 'Actual Values'])
plt.xlabel('Time')
plt.ylabel('Values')
plt.title("Data Column {}".format(col))